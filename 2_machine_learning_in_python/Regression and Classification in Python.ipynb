{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction to Regression & Classification in Python"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gr0a2iiO3WGq"
   },
   "source": [
    "# Session Overview\n",
    "\n",
    "Now that you've had some experience using R, we're going to move over to Python to look at **Machine Learning** tasks.\n",
    "\n",
    "We're going to get some practice for two common types of machine learning tasks:\n",
    "\n",
    "1. Regression \n",
    "2. Classification "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hZQGucK13WGr"
   },
   "source": [
    "### Why Python?\n",
    "The content in today's session could be accomplished in either R or Python. Python is more widely used for machine learning tasks, especially when considering neural networks (will be discussed in the following lectures). We are introducing today's material in Python to give a flavor of a different programming language, including its syntax and functionality. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "APxsG76o3WGr"
   },
   "source": [
    "# Part 0: Preparing Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some words about the packages:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**scikit-learn**:\n",
    "- A popular machine learning library for Python\n",
    "- Includes a wide variety of models, including linear and logistic regression, k-means clustering, and support vector machines.\n",
    "- Has a consistent interface to model fitting, prediction, and evaluation\n",
    "- Also has built-in functions for data preprocessing, such as feature scaling and dimensionality reduction\n",
    "\n",
    "**pandas**:\n",
    "- A powerful library for data manipulation and analysis\n",
    "- Provides data structures, such as Series and **DataFrame**, that allow for easy manipulation of data\n",
    "- Built-in functions for data cleaning, filtering, and manipulation, making it a powerful tool for data preprocessing\n",
    "- Strong integration with other popular libraries, such as NumPy and Matplotlib, for data visualization and analysis\n",
    "\n",
    "**numpy**:\n",
    "- A library for numerical computing in Python\n",
    "- Provides a powerful array object and a large collection of mathematical functions\n",
    "- Often used as the base array structure for other libraries such as Pandas and scikit-learn\n",
    "\n",
    "**seaborn**:\n",
    "- A library for data visualization in Python\n",
    "- Built on top of Matplotlib\n",
    "- Provides a high-level interface for creating informative and attractive statistical graphics\n",
    "- Specifically designed for visualizing statistical models and their results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "gq3RVZXo3WGs"
   },
   "outputs": [],
   "source": [
    "import pandas as pd # Pandas \n",
    "import numpy as np\n",
    "import random\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# surpress warnings\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "uOe5IrdG3WGs",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "## Load the data\n",
    "listings_raw = pd.read_csv('./data/listings_with_amenities.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kaPIKFLdN9wT"
   },
   "source": [
    "### A little bit of data exploration and cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "YbUQNAxm3WGs",
    "outputId": "a20e5c42-dab8-4c14-d11a-4090289de245"
   },
   "outputs": [],
   "source": [
    "## Explore the first 5 rows\n",
    "listings_raw.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ZxHYRAxdg57R",
    "outputId": "6e5c221b-6a75-496c-edb5-ab9a16a22017"
   },
   "outputs": [],
   "source": [
    "## Display the columns names\n",
    "print(listings_raw.columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "70rX1wc93WGt"
   },
   "source": [
    "Before moving forward, let's do some basic data cleaning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "JJXctR5n3WGt",
    "outputId": "bdfc1a96-b4b3-46fa-d985-039d09892f52"
   },
   "outputs": [],
   "source": [
    "def clean_price(p):\n",
    "    p = p.replace('$','').replace(',','') # replace all '$' and ',' and cast to float \n",
    "    return float(p)\n",
    "\n",
    "# We will \"apply\" this function to each element of our price column, and replace the price column with these values\n",
    "listings_raw.loc[:,'price'] = listings_raw.loc[:,'price'].apply(lambda x: clean_price(x))\n",
    "listings_raw.loc[:,'price']\n",
    "\n",
    "# Alternatively\n",
    "# listings_raw[\"price\"] = pd.to_numeric(listings_raw[\"price\"].str.replace(\"$\",\"\").str.replace(\",\",\"\"))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "S2R9P93FGh7N"
   },
   "source": [
    "Let's observe the price distribution using seaborn (more details below):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "VZglgW-IGgb8",
    "outputId": "28964153-febc-4fab-a83e-9b84bd221d5b"
   },
   "outputs": [],
   "source": [
    "sns.distplot(listings_raw[\"price\"], kde=True, color='b')\n",
    "plt.xlabel('Price')\n",
    "plt.ylabel('Frequency')\n",
    "plt.title('Distribution of Prices')\n",
    "\n",
    "\n",
    "# Alternatively: \n",
    "# sns.distplot(listings_raw[\"price\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We'll also clean up some outliers\n",
    "listings_clean = listings_raw.query('accommodates <= 10 & price <= 1000')\n",
    "listings_clean = listings_clean.query('maximum_nights <= 2000')\n",
    "\n",
    "# ALTERNATIVELY:\n",
    "# listings_clean = listings_raw[(listings_raw[\"accommodates\"] <= 10) & (listings_raw[\"price\"] <= 1000) & (listings_raw[\"maximum_nights\"] <= 2000)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter by property type\n",
    "listings_clean = listings_clean[listings_clean['property_type'].isin([\"Apartment\", \"House\", \"Bed & Breakfast\", \"Condominium\", \"Loft\", \"Townhouse\"])]\n",
    "listings_clean = listings_clean[~listings_clean['neighbourhood_cleansed'].isin([\"Leather District\", \"Longwood Medical Area\"])]\n",
    "\n",
    "# ALTERNATIVELY (You can actually use query):\n",
    "    # property_types = [\"Apartment\", \"House\", \"Bed & Breakfast\", \"Condominium\", \"Loft\", \"Townhouse\"]\n",
    "    # neighbourhoods = [\"Leather District\", \"Longwood Medical Area\"]\n",
    "    # listings_raw = listings_raw.query(\"property_type in @property_types and neighbourhood_cleansed not in @neighbourhoods\")\n",
    "\n",
    "# See the frequencies of each category of property_type\n",
    "print(listings_raw[\"property_type\"].value_counts())\n",
    "print()\n",
    "print(listings_clean[\"property_type\"].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "GwJxRnDt3WGu"
   },
   "outputs": [],
   "source": [
    "# Find columns with sufficient fill (at most 25% missing values)\n",
    "cols_keep = listings_clean.columns[listings_clean.isnull().mean() <= 0.25]\n",
    "listings_clean = listings_clean.loc[:,cols_keep]\n",
    "\n",
    "# Alternatively\n",
    "    # listings_clean = listings_raw.dropna(thresh=len(listings_raw)*0.25, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's one-hot encode our property types/categorical variables\n",
    "listings_clean = pd.get_dummies(listings_clean, columns = ['property_type'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lets take a look at the dummy variables btw\n",
    "property_type_columns = listings_clean[[col for col in listings_clean.columns if \"property_type\" in col]]\n",
    "property_type_columns.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We're also going to filter to numeric columns for now- \n",
    "# You can use one-hot encoding to handle categorical variables, but today we'll stick to this.\n",
    "listings_clean = listings_clean.select_dtypes(include=np.number)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print important statistics\n",
    "listings_clean.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "f3yiVsGiKqWW"
   },
   "source": [
    "### Back to the Sea(born)! \n",
    "\n",
    "Check out for more ideas there: https://seaborn.pydata.org/introduction.html\n",
    "\n",
    "Here are some examples for our dataset.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Create a 2x2 subplot\n",
    "fig, ax = plt.subplots(2, 2, figsize=(15, 10))\n",
    "\n",
    "# Create the first distribution plot in the first subplot\n",
    "sns.distplot(listings_clean['price'], ax=ax[0, 0])\n",
    "ax[0, 0].set_title(\"Price Distribution\")\n",
    "\n",
    "# Create the second distribution plot in the second subplot\n",
    "sns.distplot(listings_clean['accommodates'], ax=ax[0, 1])\n",
    "ax[0, 1].set_title(\"Accommodates Distribution\")\n",
    "\n",
    "# Create the third distribution plot in the third subplot\n",
    "sns.distplot(listings_clean['number_of_reviews'], ax=ax[1, 0])\n",
    "ax[1, 0].set_title(\"Number of Reviews Distribution\")\n",
    "\n",
    "# Create the fourth distribution plot in the fourth subplot\n",
    "sns.distplot(listings_clean['review_scores_rating'].dropna(), ax=ax[1, 1])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Alternatively - use for loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Create a 2x2 subplot\n",
    "fig, ax = plt.subplots(2, 2, figsize=(15, 10))\n",
    "\n",
    "# List of columns\n",
    "columns = ['price', 'accommodates', 'number_of_reviews', 'review_scores_rating']\n",
    "numeric = listings_clean[columns]\n",
    "\n",
    "# Loop through the columns and create the distribution plots\n",
    "for i, column in enumerate(columns):\n",
    "    sns.distplot(listings_clean[column].dropna(), ax=ax[i//2, i%2])\n",
    "    ax[i//2, i%2].set_title(f\"{column} Distribution\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# create box plots for outlier detection for numerical data\n",
    "numeric = listings_clean[['price', 'accommodates', 'number_of_reviews']]\n",
    "fig, axes = plt.subplots(nrows=len(numeric.columns), ncols=1, figsize=(5,20))\n",
    "for i, column in enumerate(numeric.columns):\n",
    "    sns.boxplot(data=listings_clean,y=column,orient=\"v\",ax=axes[i],palette=\"Greens\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Jgs1mXUnJjUI",
    "outputId": "530988db-aa6d-4783-9540-54eab99cf878"
   },
   "outputs": [],
   "source": [
    "sns.catplot(data=listings_clean[['property_type_House', 'accommodates', 'price']], kind=\"swarm\", x=\"accommodates\", y=\"price\", hue=\"property_type_House\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "eFrVh4fjK-Wu"
   },
   "source": [
    "### Missing Data\n",
    "\n",
    "It is important to check for missing data and handle it properly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# List of values to be replaced with np.na\n",
    "replace_values = ['', ' ', '#VALUE!', 'NA', 'N/A', 'nan', 'NaN']\n",
    "listings_clean = listings_clean.replace(replace_values, np.nan)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "EssjMwfi3WGu",
    "outputId": "421f1043-4bd4-4a9b-9f14-79f9cb449270"
   },
   "outputs": [],
   "source": [
    "# Where are there missing values?\n",
    "missing_vals = listings_clean.isna().sum()\n",
    "missing_vals.index[missing_vals>0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One cool way to visualize missing values is by using the missingno library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import missingno as ms\n",
    "mv_matrix = ms.matrix(listings_clean,figsize=(40, 10), color = (0.1, 0.4, 0.1), labels=True) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4OJCBW_53WGv"
   },
   "source": [
    "Let's impute the missing data - we will simply impute the mean. There are other options available in `sklearn.impute`.\n",
    "Notice that the missingness is in numeric columns only, which makes it easier to deal with. If there are categoric columns with missing data, different approaches need to be taken (e.g. mode imputation)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "aWINKGnl3WGv"
   },
   "outputs": [],
   "source": [
    "from sklearn.impute import SimpleImputer\n",
    "\n",
    "imp_mean = SimpleImputer(missing_values=np.nan, strategy='mean')\n",
    "imp_mean.fit(listings_clean)\n",
    "\n",
    "# Get the imputed values and convert back into a dataframe (it will return a matrix)\n",
    "listings = pd.DataFrame(imp_mean.transform(listings_clean), \n",
    "                        columns = listings_clean.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ALTERNATIVELY, SINCE MEAN IS A SIMPLE STRATEGY:\n",
    "    # listings = listings_clean.fillna(listings_clean.mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Please note that imputing missing values with the mean value can change the distribution of the data and can affect the results of your analysis, it's important to keep this in mind when choosing the imputation method.**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "orRiAQHS3WGv"
   },
   "source": [
    "Let's see what our feature space looks like now.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ms.matrix(listings,figsize=(40, 10), color = (0.1, 0.4, 0.1), labels=True) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "J5L69MRS3WGv"
   },
   "source": [
    "# Part 1: Regression\n",
    "\n",
    "**In simple terms**\n",
    "\n",
    "Regression is a statistical method that is used to find the relationship between a dependent variable (also called the outcome or response variable) and one or more independent variables (also called predictors or explanatory variables).\n",
    "\n",
    "In simple linear regression, there is only one independent variable, and the relationship between the independent variable and the dependent variable is represented by a straight line. The equation of the line is:\n",
    "\n",
    "$y = b_0 + b_1x$\n",
    "\n",
    "Where:\n",
    "\n",
    "- y is the dependent variable\n",
    "- x is the independent variable\n",
    "- $b_0$ is the y-intercept of the line (the value of y when x = 0)\n",
    "- $b_1$ is the slope of the line (the change in y for a unit change in x)\n",
    "\n",
    "In multiple linear regression, there are two or more independent variables, and the relationship between the independent variables and the dependent variable is represented by a hyperplane. The equation of the hyperplane is:\n",
    "\n",
    "$y = b_0 + b_1x_1 + b_2x_2 + ... + b_nx_n$\n",
    "\n",
    "Where:\n",
    "\n",
    "- y is the dependent variable\n",
    "- $x_1, x_2, ..., x_n$ are the independent variables\n",
    "- $b_0, b_1, b_2, ..., b_n$ are the coefficients of the model\n",
    "\n",
    "The goal of regression is to find the best-fitting line or hyperplane that describes the relationship between the dependent and independent variables. The best-fitting line or hyperplane is the one that minimizes the sum of the squared differences between the predicted values and the actual values."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**We're going to start by solving a regression problem.**\n",
    "\n",
    "**Inputs**\n",
    "\n",
    "Regression is a supervised learning problem, which means that we have access to a dataset $\\mathcal{D} = \\{ (x_i, y_i) \\}_{i = 1}^{n}$ where each $x_i \\in \\mathbb{R}^d$ and each $y_i \\in \\mathbb{R}$.\n",
    "\n",
    "Note that for regression, the output, $y$, is **continuous**. If the output is discrete, such as whether or not an apartment listing will be rented (yes/no) or what town the unit is listed in (multiple classes: Boston vs. Cambridge vs. Somerville), we would use a *classification* algorithm. We will consider this in part 2 of the session.\n",
    " \n",
    "**Aim**\n",
    "\n",
    "The aim of a regression task is to find a function $h: \\mathbb{R}^d \\to \\mathbb{R}$ which allows us to compute an accurate prediction of the output, $y$, for any given input, $x$. These new $x$ and $y$ are *unseen*: that is, we assume they are identically distributed relative to the points in $\\mathcal{D}$, but independent.\n",
    "\n",
    "Exactly what we mean by *accuracy* will be discussed when we talk about model evaluation.\n",
    "\n",
    "The way we approach the problem of finding $h$ is by selecting a parameterised class of models, and solving an optimization problem to find the best model within this class according to a *loss function*. We usually repeat this process for several model classes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FAbR9wER3WGw"
   },
   "source": [
    "### Defining our Regression Task\n",
    "\n",
    "We're going to use the Boston Airbnb dataset, where each entry is a property listing, and aim to use the input variables available to us in order to predict the price of a listing.\n",
    "\n",
    "Note here that we haven't specified exactly which variables in the dataset will be used to define each $x_i$. This choice of **feature space** is part of the modeling process, and we will experiment with it."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "81MyBXe33WGw"
   },
   "source": [
    "### Ordinary Least Squares\n",
    "\n",
    "The first class of models we're going to try is linear models. This means we hypothesise that the output, $y$, can be described **using a linear combination of the inputs**, $x$: $h(x) = \\beta^{\\intercal} x$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9hFpSUIY3WGw"
   },
   "source": [
    "#### Fitting an OLS Model \n",
    "\n",
    "Now, we need to choose exactly which inputs will be used for fitting the model. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "EO3KMX293WGw"
   },
   "source": [
    "For a very simple model, let's just choose `accommodates` as the only input. To fit this model we will use linear regression in scikit-learn:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "mXFjGOvR3WGw",
    "outputId": "2fd55745-54c4-4e6a-d3ce-4f2156c98fc3",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "X = listings[['accommodates']]\n",
    "y = listings['price']\n",
    "\n",
    "# Select the model type\n",
    "model = LinearRegression()\n",
    "\n",
    "# Fit the model to our data\n",
    "model.fit(X, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fN8WI5sk3WGw"
   },
   "source": [
    "We've trained our first model! Let's look at the model output. You can see the elements of the `model` object by typing `model.` and pressing \"tab.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "pMXbVep13WGw",
    "outputId": "2cb23f61-c549-4c8d-eb28-31bf23566c11"
   },
   "outputs": [],
   "source": [
    "print(model.coef_)\n",
    "print(model.intercept_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "70gbiR0c3WGx"
   },
   "source": [
    "Let's look at the 'coefficients' section. In the 'estimate' column, we see that the point estimates for the model coefficients say that the price estimate is \\\\$53.67 + \\\\$38.18*(# of people accommodated). \n",
    "\n",
    "To visualise the model, let's plot the fitted line. We'll use `matplotlib` for our plotting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "-242FGoc3WGx",
    "outputId": "1b881437-0437-44f9-da25-b9dbb2b24be9"
   },
   "outputs": [],
   "source": [
    "# Start with a scatterplot of the data\n",
    "plt.plot(X, y, 'o')\n",
    "\n",
    "# Add in the regression line\n",
    "y_pred = model.predict(X)\n",
    "plt.plot(X, y_pred)\n",
    "plt.title('OLS Model Fit')\n",
    "plt.xlabel(\"Accomodates\")\n",
    "plt.ylabel(\"Price\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pincTs1Z3WGx"
   },
   "source": [
    "Let's look the model residuals for each accommodation size to see how appropriate the linear model is for this problem. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Wnq-ytlg3WGx"
   },
   "source": [
    "\n",
    "### Evaluation\n",
    "\n",
    "**Mean Squared Error (MSE)**\n",
    "\n",
    "Mean Squared Error (MSE) is a measure of the average squared difference between the predicted and actual values. It is calculated using the following equation:\n",
    "\n",
    "$MSE = \\frac{1}{n}\\sum_{i=1}^n (y_i - \\hat{y_i})^2$\n",
    "\n",
    "Where:\n",
    "\n",
    "- $y_i$ is the actual value of the dependent variable\n",
    "- $\\hat{y_i}$ is the predicted value of the dependent variable\n",
    "- $\\bar{y_i}$ is the mean of the dependent variable\n",
    "- n is the number of observations\n",
    "\n",
    "**R-squared ($R^2$)**\n",
    "\n",
    "R-squared is a statistical measure of how close the data are to the fitted regression line. It is a value between 0 and 1, where 1 represents a perfect fit, and 0 represents no fit. R-squared is calculated using the following equation.\n",
    "\n",
    "It represents the proportion of the variance for a dependent variable that's explained by an independent variable or variables in a regression model.  It is defined as:\n",
    "\n",
    "$R^2 =1- \\frac{SS_{res}}{SS_{tot}} = 1 - \\frac{\\sum_{i=1}^n(y_i-\\hat{y}_i)^2}{\\sum_{i}(y_i-\\bar{y})^2}$\n",
    "\n",
    "\n",
    "\n",
    "Intuitively, R2 measures how accurate our model predictions are in comparison to a **baseline model** which\n",
    "predicts the **mean of y** from the **training set**.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Mean squared error (MSE) Calculation**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "_ufb4YVL3WGx",
    "outputId": "0819b2f6-c2f4-4d69-8f21-d5f9e95abe18"
   },
   "outputs": [],
   "source": [
    "resid = (y - y_pred)\n",
    "mse = np.mean(resid**2)\n",
    "mse"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "02nx-aXv3WGy"
   },
   "source": [
    "We can also calculate the RMSE, (root) MSE, from a model applied to a dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "hEvvUEOM3WGy",
    "outputId": "9161c0f1-16d5-41f9-ef06-b3bd10b1021d",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "rmse = np.sqrt(np.mean(resid**2))\n",
    "rmse"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "oGtOL0iX3WGy"
   },
   "source": [
    "Scikit-learn has built-in evaluation functions that can do this for you. We'll also print the results out using Python's number formatting to show the MSE/RMSE rounded to two decimals."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "iG60pJHs3WGy",
    "outputId": "2400de32-bc3a-44d7-b210-4a8ebb83138a"
   },
   "outputs": [],
   "source": [
    "from sklearn import metrics\n",
    "mse = metrics.mean_squared_error(y, y_pred)\n",
    "rmse = metrics.mean_squared_error(y, y_pred)**(1/2)\n",
    "print(\"MSE = %.2f\" % mse)\n",
    "print(\"RMSE = %.2f\" % rmse)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "420-rPRC3WGy"
   },
   "source": [
    "Clearly this measure is highly affected by the scale of the data. We can also use the 'R squared' coefficient, which is more interepretable since it is on a standardized scale."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**R-squared calculation**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "3XsMENnX3WGy",
    "outputId": "7f023cbc-017a-4934-98fa-2b00179fafce",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def r_squared(y_true, y_pred, y_mean):\n",
    "    ss_res = ((y_true - y_pred)**2).sum()\n",
    "    ss_tot = ((y_true - y_mean)**2).sum()\n",
    "    return (1 - (ss_res/ss_tot))\n",
    "\n",
    "r2 = r_squared(y, y_pred, np.mean(y))\n",
    "print(\"R^2 = %.2f\" % r2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wvqeTWpP3WGz"
   },
   "source": [
    "\n",
    "The R squared value is what we will use to evaluate the performance of our models in this session. But, it's important to note that this is definitely not the only choice we could have made! Check out scikit-learn's full library of [evaluation metrics](https://scikit-learn.org/stable/modules/classes.html#module-sklearn.metrics) to see what else is out there.\n",
    "\n",
    "**Question**: What is the relationship between the **loss function** and the **measure of accuracy** used to evaluate a model? How do we know which is the best choice for each? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cellView": "form",
    "id": "QHDm0iIhSDGP"
   },
   "source": [
    "#### Answer\n",
    "- The loss function is used to fit a model, and the measure of accuracy is used to evaluate how well it explains the data. \n",
    "- The measure of accuracy usually comes from the task we aim to solve, and the loss function is often something that we will play around with until we find a good model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GZp_33PS3WGz"
   },
   "source": [
    "#### Training, Validation and Testing Splits\n",
    "\n",
    "Recall that our ultimate goal in this supervised learning task is to be able to predict price ($y$) from an **unseen** set of inputs ($x$, although we are still playing around with the input variables which define it).\n",
    "\n",
    "When building the OLS model, we used the entire dataset. Simply taking the R squared value on this dataset as a measure of performance is clearly not fair -- since we want the model to generalise to unseen data.\n",
    "\n",
    "To address this problem, we often split the dataset into three different chunks:\n",
    "\n",
    "1. **Training data**: the data we build our models on.\n",
    "2. **Validation data**: the data we tune hyperparameters on.\n",
    "3. **Testing data**: the data we use to obtain a final estimate of performance.\n",
    "\n",
    "The validation set is like an 'intermediate' estimate of performance. If we didn't use the validation set for this purpose, the only way of selecting the best model from a model class would be to look at its performance on the training set or the testing set.\n",
    "\n",
    "`train_test_split` in scikit-learn provides an easy way of creating these data partitions. Let's start by just creating training/testing sets. We'll get to parameter tuning in a minute."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "f_HxBdmv3WGz",
    "outputId": "cee421c0-fa59-41c1-84ad-4a1a7eb9fd80",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "features =  listings.columns.drop(['id', 'scrape_id', 'host_id', 'latitude', 'longitude', \n",
    "                                   'availability_30','availability_60', 'availability_90', 'availability_365',\n",
    "                                   'host_total_listings_count','calculated_host_listings_count',\n",
    "                                   'price'])\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(listings[features], listings['price'], \n",
    "                                                    train_size = 0.75, random_state = 1)\n",
    "\n",
    "print(\"Training Size = %d\" % X_train.shape[0]) # To print integers, we can pass '%d' into our print statement \n",
    "print(\"Testing Size = %d\" % X_test.shape[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PxbtmoFnMwxz"
   },
   "source": [
    "Let's now observe our training data and do some basic pre-processing before training the models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "memr9EtFjRGu",
    "outputId": "2d304243-4a4f-4f9c-f0c0-45ac31fd8be6"
   },
   "outputs": [],
   "source": [
    "# Describe the data \n",
    "X_train.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "id": "rkH6FNkQNHNV"
   },
   "outputs": [],
   "source": [
    "# We should normalize or standardize our data, so that features are on the same scale.\n",
    "\n",
    "### The \"manual\" way ###\n",
    "\n",
    "# Normalization\n",
    "X_train_normalized = (X_train - X_train.min())/(X_train.max()-X_train.min())\n",
    "X_test_normalized = (X_test - X_train.min())/(X_train.max()-X_train.min())\n",
    "\n",
    "# Standardization\n",
    "X_train_standardized = (X_train - X_train.mean())/X_train.std()\n",
    "X_test_normalized = (X_test - X_train.mean())/X_train.std()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "id": "rkH6FNkQNHNV"
   },
   "outputs": [],
   "source": [
    "### The \"package\" way ###\n",
    "\n",
    "# Normalization\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "norm_scaler = MinMaxScaler()\n",
    "norm_scaler.fit(X_train)\n",
    "X_train_normalized2 = norm_scaler.transform(X_train)\n",
    "X_test_normalized2 = norm_scaler.transform(X_test)\n",
    "\n",
    "\n",
    "# Standardization\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "std_scaler = StandardScaler()\n",
    "std_scaler.fit(X_train)\n",
    "X_train_standardized2 = std_scaler.transform(X_train)\n",
    "X_test_standardized2 = std_scaler.transform(X_test)\n",
    "\n",
    "# You can look at the mean and std from the scaler\n",
    "# print(std_scaler.mean_)\n",
    "# print(std_scaler.scale_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wm5b35Ls3WGz"
   },
   "source": [
    "\n",
    "### Model Iteration\n",
    "\n",
    "Now that we're equipped to build models and evaluate their performance, let's start iterating to find better models.\n",
    "\n",
    "We've glossed over the precise choice of variables to use in order to explain price, so let's try a few different combinations. \n",
    "\n",
    "Now we'll write a function that accepts data (X,y) and splits into training/testing data, fits a model, and returns the R squared value on each partition."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "pYsssdnK3WGz"
   },
   "outputs": [],
   "source": [
    "def evaluate(model, X_train, y_train, X_test, y_test):\n",
    "    y_train_pred = model.predict(X_train)\n",
    "    y_test_pred = model.predict(X_test)\n",
    "    \n",
    "    rmse_train = metrics.mean_squared_error(y_train, y_train_pred)**(1/2)\n",
    "    rmse_test = metrics.mean_squared_error(y_test, y_test_pred)**(1/2)\n",
    "    \n",
    "    print(\"Train RMSE = %.2f\" % rmse_train)\n",
    "    print(\"Test RMSE = %.2f\" % rmse_test)\n",
    "    \n",
    "    rsq_train = r_squared(y_train, y_train_pred, np.mean(y_train))\n",
    "    rsq_test = r_squared(y_test, y_test_pred, np.mean(y_train))\n",
    "    \n",
    "    print(\"Train R^2 = %.2f\" % rsq_train)\n",
    "    print(\"Test R^2 = %.2f\" % rsq_test)\n",
    "\n",
    "def train_eval(model, X, y, split_seed = 1):\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, train_size = 0.75, random_state = split_seed)\n",
    "    \n",
    "    # we will perform Min-Max Scaling\n",
    "    norm_scaler = MinMaxScaler()\n",
    "    norm_scaler.fit(X_train)\n",
    "    X_train_normalized2 = norm_scaler.transform(X_train)\n",
    "    X_test_normalized2 = norm_scaler.transform(X_test)\n",
    "    \n",
    "    model = model.fit(X_train, y_train)\n",
    "    evaluate(model, X_train, y_train, X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "tmVMfKV53WGz",
    "outputId": "2d160486-e6c0-46b2-eb28-5155375b23ac"
   },
   "outputs": [],
   "source": [
    "print(\"Price ~ Accomodates\")\n",
    "train_eval(LinearRegression(), listings[['accommodates']], listings['price'])\n",
    "print(\"\\nPrice ~ Accomodates + Reviews/Month\")\n",
    "train_eval(LinearRegression(), listings[['accommodates','reviews_per_month']], listings['price'])\n",
    "print(\"\\nPrice ~ Accomodates + Reviews/Month + Rating\")\n",
    "train_eval(LinearRegression(), listings[['accommodates','reviews_per_month','review_scores_rating']], listings['price'])\n",
    "print(\"\\nPrice ~ All Variables\")\n",
    "train_eval(LinearRegression(), listings[features], listings['price'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cFmb8t5n3WGz"
   },
   "source": [
    "\n",
    "**Question**: Can we say anything about these results?\n",
    "\n",
    "**Answer**: We can be relatively confident that the largest model is the best performing. We also see that the testing performance is significantly worse than the training performance. Usually this indicates overfitting."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1WzMOXyT3WG0"
   },
   "source": [
    "### Regularization\n",
    "\n",
    "In some situations, overfitting is more obvious -- and since it is a common problem when building models, we will now look at how it can be addressed.\n",
    "\n",
    "Regularization is a tool which helps us to avoid overfitting by penalising model complexity. Mathematically, we add a term to the loss function in the optimization problem to be solved. Recall that the OLS formulation we've worked with is:\n",
    "\n",
    "$$\\min_w \\: \\frac{1}{n} \\sum_{i = 1}^{n} (w^\\intercal x_i - y_i)^2$$\n",
    "\n",
    "With a regularization term, this becomes:\n",
    "\n",
    "$$\\min_w \\: \\frac{1}{n} \\sum_{i = 1}^{n} (w^\\intercal x_i - y_i)^2 + \\lambda \\Omega(w)$$\n",
    "\n",
    "\n",
    "$\\Omega(w)$ is a penalty on the complexity of the model. Two common choices for $\\Omega(w)$ are:\n",
    "\n",
    "1. $\\Omega(w) = ||w||_2^2 = \\sqrt{\\sum_{i=1}^n w_i^2} $: this is Ridge regression.\n",
    "2. $\\Omega(w) = ||w||_1 = \\sum_{i=1}^n |w_i|$: this is Lasso regression.\n",
    "\n",
    "Both types of regression shrink the elements of the optimal $w^*$ vector towards 0 -- but in different ways. We will focus on Lasso -- which tends to shrink the coefficients so that some are equal to 0. Yes, we all know Lasso is not a sparse method, but \"often\" it leads to sparser solutions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XmOd_YEa3WG0"
   },
   "source": [
    "### Cross-Validation\n",
    "\n",
    "We need a way of selecting the one with the best parameter for our task.\n",
    "\n",
    "**Question**: How would we do this using the training/validation/testing splits?\n",
    "\n",
    "**Answer**: Fit models on the training set, varying lambda. Then obtain an estimate of performance for each model on the validation set, and choose the best. Obtain a final estimate of performance for this model on the testing set.\n",
    "\n",
    "Here, because `GridSearchCV` makes it easy, we're going to use a similar technique called cross-validation. Generally, we only consider this necessary when we're worried we have too little training data to obtain an accurate estimate of validation performance.\n",
    "\n",
    "The idea behind cross-validation is: repeating the training/validation process multiple times provides us with several estimates of validation performance. Taking the average of these hopefully gives us an estimate which is less affected by noise.\n",
    "\n",
    "To cross-validate, we start with only two partitions of the dataset:\n",
    "\n",
    "1. Combined training/validation set: the data that we repeatedly train and validate on.\n",
    "2. Testing set: the data we obtain our final performance estimate on.\n",
    "\n",
    "But, how do we train and validate?\n",
    "\n",
    "- First, select a number of *folds*.\n",
    "- Then divide the training/validation data into this number of equal-sized partitions.\n",
    "- For each fold, repeat the training/validation procedure. The fold is the validation data, and the other folds are training data.\n",
    "- Average the performance of each model across folds and pick the hyperparameters which produce the best model.\n",
    "- Fit a model on the entire training set using the selected values of hyperparameters.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](https://miro.medium.com/max/1838/1*AAwIlHM8TpAVe4l2FihNUQ.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MyYLO9bE3WG0"
   },
   "source": [
    "We'll use our training set from before and run 5-fold cross-validation to determine the best `alpha`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question**: \n",
    "- Why is validation an important step before testing? \n",
    "- When selecting the best model based on validation, how do we expect the test evaluation to behave?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "4sIgGnaO3WG0",
    "outputId": "e2fd9895-b7a0-43a2-8127-8d053531e7fb",
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from sklearn.linear_model import Lasso\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "# Define the grid that we want to search over: np.arange allows you to create a list of values you want to cross-validate\n",
    "grid = {'alpha':np.arange(0.01, 1, 0.01)}\n",
    "\n",
    "# Define the parameters for the model \n",
    "gs = GridSearchCV(\n",
    "    estimator = Lasso(max_iter=1000000), \n",
    "    param_grid = grid, \n",
    "    cv=5)\n",
    "\n",
    "## Fit the model\n",
    "random.seed(1)\n",
    "gs.fit(X_train, y_train)\n",
    "m = gs.best_estimator_\n",
    "\n",
    "## Print the best parameters determined by the cross-validation\n",
    "print(\"Alpha: %.3f\" % gs.best_params_['alpha'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "n6hGH3lf3WG0",
    "outputId": "15575381-d8c3-46b4-92d4-300c3017b3d8"
   },
   "outputs": [],
   "source": [
    "coef_output = pd.DataFrame({'feature':X_train.columns,\n",
    "              'coefficient':m.coef_})\n",
    "print(\"Number of zeros: %d\" % (coef_output.query('coefficient == 0').shape[0]))\n",
    "coef_output.sort_values('coefficient')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zyG539gO3WG1"
   },
   "source": [
    "We still haven't obtained an error measurement on the testing data, which will allow us to compare our best regularised model with the unregularised one. \n",
    "\n",
    "Let's generate predictions and calculate our performance metrics:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "zMbV3WVO3WG1",
    "outputId": "1471c3ab-8e77-41ba-c9d2-d3ed1122adef",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "evaluate(gs, X_train, y_train, X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wPteX88S3WG1"
   },
   "source": [
    "Let's see how well we do if we use a higher alpha parameter, even though its slightly suboptimal."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "MWNJgLck3WG1",
    "outputId": "59aa1daf-c3b8-46d5-bb58-91c9ad7ad9c7"
   },
   "outputs": [],
   "source": [
    "clf = Lasso(alpha=0.5)\n",
    "clf.fit(X_train, y_train)\n",
    "evaluate(clf, X_train, y_train, X_test, y_test)\n",
    "print(\"Number of zeros: %d\" % sum(clf.coef_ == 0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zTe0tBqz3WG1"
   },
   "source": [
    "The previous best model had a test RMSE of 82.64 (and $R^2$ of 0.41), so we now have a slightly better model. Regularization has also given us a way of obtaining an accurate model with fewer nonzero coefficients. Even when taking the higher suboptimal alpha, we still do nearly as well with even higher interpretability (5 fewer nonzeros than the optimal Lasso model). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cXTP_RXKZ5w3"
   },
   "source": [
    "## Other Regressors!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tKMtIFGrS94t"
   },
   "source": [
    "Until now we have worked on Linear Regressors. Let's expand see how other tree based models perform!\n",
    "\n",
    "### Decision Trees & Random Forests\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Decision Trees \n",
    "We will briefly explore regression trees (often referred to as CART, for Classification And Regression Trees).\n",
    "\n",
    "A tree makes predictions by grouping similar observations and then averaging the observations or using a linear regression model for the group. Groups can be thought of as nodes on a tree, and tree branches correspond to logical criteria on the predictor variables. There's a lot of neat math that goes into building the trees, but we won't get into that today. For now let's get familiarized by looking at a simple example. We will use the `DecisionTreeRegressor` in scikit-learn.\t\n",
    "\n",
    "#### Tuning the CART model\n",
    "If we want to construct high accuracy decision tree models, then we need to tune the parameters.  CART has many parameters that specify how the decision tree is constructed. Today we'll focus on `max_depth`. \n",
    "* If the tree is too big => too many splits => we have an over-fitting problem.\n",
    "* If the tree is too small => too few splits => we have an under-fitting problem.\n",
    "\n",
    "We want to find a tree in the middle, that is \"just right\". We'll use `GridSearchCV` package to tune the tree depth using cross-validation. \n",
    "\n",
    "The model construction step follows the same established pattern."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeRegressor\n",
    "\n",
    "params = {\n",
    "    'max_depth' : [i for i in range(1,10,1)]\n",
    "}\n",
    "\n",
    "model = GridSearchCV(\n",
    "        DecisionTreeRegressor(),\n",
    "        params,\n",
    "        cv=5,\n",
    "        verbose=1\n",
    ")\n",
    "\n",
    "model.fit(X_train, y_train)\n",
    "evaluate(model, X_train, y_train, X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lets plot the tree\n",
    "from sklearn.tree import plot_tree\n",
    "\n",
    "fig, axes = plt.subplots(nrows = 1,ncols = 1,figsize = (50,10), dpi=200)\n",
    "plot_tree(model.best_estimator_, feature_names = X_train.columns, filled = False, fontsize = 8);\n",
    "fig.savefig('cart_tree.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random Forests\n",
    "\n",
    "We will briefly take a look at random forests, using the [RandomForestRegressor()](https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestClassifier.html). A random forest is a collection of slightly randomized decision trees (hence the name \"forest\"), and can be used for classification or regression. They often have excellent predictive performance, but can be expensive to train and lack interpretability. Random forests have many hyperparameters that can be tuned to achieve the best possible predictive performance. Perhaps the most important hyperparameter is the number of trees to include in the forest. More trees results in a longer training time but can improve prediction and decrease overfitting.\n",
    "The goal of Random Forest is to train a pool of base tree learners with a high variance but a small bias. They are not necessarily weak learners!\n",
    "With bagging, we explicitly train weaker (but not necessarily weak) learners."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestRegressor\n",
    "\n",
    "params = {\n",
    "    'n_estimators' : [100, 200],\n",
    "    'max_depth' : [i for i in range(1,10,2)]\n",
    "}\n",
    "\n",
    "model = GridSearchCV(\n",
    "        RandomForestRegressor(random_state=42),\n",
    "        params,\n",
    "        cv=5,\n",
    "        verbose=1\n",
    ")\n",
    "\n",
    "model.fit(X_train, y_train)\n",
    "evaluate(model, X_train, y_train, X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Boosted Trees\n",
    "\n",
    "Boosting means combining a learning algorithm in series to achieve a strong learner from many sequentially connected weak learners. In case of gradient boosted decision trees algorithm, the weak learners are decision trees.\n",
    "\n",
    "Each tree attempts to minimize the errors of the previous trees. Trees in boosting are weak learners but adding many trees in series and each focusing on the errors from previous one make boosting a highly efficient and accurate model.\n",
    "\n",
    "A major difference between random forests and boosting algorithms is the overall size of the trees. In a random forest, we tend to choose trees as strong learners — they would do just fine as a decision tree on their own. In boosting algorithms, trees are artificially limited to very shallow depths (in practice rarely more than depth 5), to ensure that each model is only slightly better than random chance.\n",
    "\n",
    "Let' work with the famous XGBoost, maybe the most widely used machine learning model!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from xgboost.sklearn import XGBRegressor\n",
    "params = {\n",
    "    'n_estimators' : [50, 100],\n",
    "    'max_depth' : [2, 4, 6, 8, 10]\n",
    "}\n",
    "\n",
    "model = GridSearchCV(\n",
    "        XGBRegressor(random_state=42),\n",
    "        params,\n",
    "        cv=5,\n",
    "        verbose=1\n",
    ")\n",
    "\n",
    "model.fit(X_train, y_train)\n",
    "evaluate(model, X_train, y_train, X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](https://www.datascienceland.com/media/uploads/2020/11/29/difference.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ntMhsGa63WG1"
   },
   "source": [
    "# Part 2: Classification\n",
    "So far we've looked at models which predict a continuous response variable. There are many related models which predict categorical outcomes, such as whether an email is spam or not, or which digit a handwritten number is. We'll take a look at some of these. Let's begin with Logistic Regression!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "iNM_A4gI3WG1"
   },
   "source": [
    "### Logistic Regression\n",
    "Logistic regression is part of the class of generalized linear models (GLMs), which build directly on top of linear regression. These models take the linear fit and map it through a non-linear function. \n",
    "\n",
    "Specifically, logistic regression is used when the dependent variable is **binary** (e.g. true/false, yes/no, 0/1) and the goal is to find the relationship between one or more independent variables and the probability of the dependent variable being true or 1.\n",
    "\n",
    "For logistic regression, the function $f(\\cdot)$ is given by $\\displaystyle f(z) = \\frac{1}{1+\\exp(-z)}$ (the *logistic function* - also called the *sigmoid function*).\n",
    "\n",
    "\n",
    "Where:\n",
    "\n",
    "- $f(z)$ is the probability of the dependent variable being true or 1\n",
    "- z is the linear combination of the independent variables and the coefficients of the model: \n",
    "$\\displaystyle z = b_0 + b_1x_1 + b_2x_2 + ... + b_nx_n$\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "M4ezb32D3WG2"
   },
   "source": [
    "Let's apply a logistic regression model to the `listings` data. Let's try to predict which listings have elevators in the building by using `price` as a predictor. Remember that our `amenity_Elevator_in_Building` variable already stores our binary outcome of interest (1 = elevator, 0 = no elevator).\n",
    "\n",
    "We need to re-split our data with y as the `amenity_Elevator_in_Building` variable. We will also *stratify* our split on this variable, meaning that the data is split to preserve the ratio of units with elevators in both the training and testing sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "NSwDOBxp3WG2"
   },
   "outputs": [],
   "source": [
    "features =  listings.columns.drop(['id', 'scrape_id', 'host_id', 'latitude', 'longitude', \n",
    "                                   'host_total_listings_count','calculated_host_listings_count',\n",
    "                                   'availability_30','availability_60', 'availability_90', 'availability_365',\n",
    "                                   'amenity_Elevator_in_Building'])\n",
    "X_train, X_test, y_train, y_test = train_test_split(listings[features], listings['amenity_Elevator_in_Building'], \n",
    "                                                    train_size = 0.7, random_state = 42,\n",
    "                                                   stratify = listings['amenity_Elevator_in_Building'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mx0X_H8u3WG2"
   },
   "source": [
    "Instead of linear regression, we'll now use `LogisticRegression()`, but the syntax is very similar. We'll skip ahead directly to using a grid search to tune regularization parameters. As you can see, the same steps apply even with a new model:\n",
    "1. Define a parameter grid\n",
    "2. Initialize a GridSearchCV object with the tuning grid and other fixed parameters\n",
    "3. Fit the model!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Cox4GViC3WG2",
    "outputId": "7f1791a5-e361-4904-eb0e-12ed6f0925d1",
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "# Define the grid that we want to search over\n",
    "param_grid = {'C': np.arange(0.001, 1, 0.05), \n",
    "              'penalty': ['l2','l1'], \n",
    "              'solver': ['liblinear']}\n",
    "\n",
    "# Define the parameters for the model \n",
    "gs = GridSearchCV(LogisticRegression(random_state=42, max_iter = 1000),\n",
    "                  return_train_score=True, \n",
    "                  param_grid=param_grid, \n",
    "                  scoring='roc_auc',\n",
    "                  cv=5, verbose = 0)\n",
    "## Fit the model\n",
    "random.seed(42)\n",
    "gs.fit(X_train, y_train)\n",
    "m_lr = gs.best_estimator_\n",
    "print(\"Best parameters: \", gs.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "xMQrXD1r3WG2",
    "outputId": "011731b0-1156-4fb1-bc25-a255c879d5ae"
   },
   "outputs": [],
   "source": [
    "# Let's look at the coefficients\n",
    "coef_output = pd.DataFrame({'feature':X_train.columns,\n",
    "              'coefficient':m_lr.coef_[0]})\n",
    "\n",
    "print(\"Number of zeros: %d\" % (coef_output.query('coefficient == 0').shape[0]))\n",
    "coef_output.sort_values('coefficient')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Ocg30NLm3WG2"
   },
   "source": [
    "We can now explore out-of-sample performance. Ultimately, we want to predict whether or not a listing has an elevator. However, logistic regression gives us something a bit different: a probability that each listing has an elevator. This gives us flexibility in the way we predict. The most natural thing would be to predict that any listing with predicted probability above 0.5 *has* an elevator, and any listing with predicted probability below 0.5 *does not have* an elevator. But what if I use a wheelchair and I want to be really confident that there's going to be an elevator? I may want to use a cutoff value of 0.9 rather than 0.5. In fact, we could choose any cutoff value and have a corresponding prediction model.\t\n",
    "\n",
    "This is where AUC comes in. For every cutoff, we'll plot the *false positive rate* against the *true positive rate* and then take the area under this curve.\t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "lupSRSJA3WG2",
    "outputId": "6e730c84-2660-4837-9908-4eb934f78664"
   },
   "outputs": [],
   "source": [
    "train_pred = m_lr.predict_proba(X_train)[:,1]\n",
    "test_pred = m_lr.predict_proba(X_test)[:,1]\n",
    "\n",
    "print(\"Train AUC: %.3f\" % metrics.roc_auc_score(y_train, train_pred))\n",
    "print(\"Test AUC: %.3f\" % metrics.roc_auc_score(y_test, test_pred))\n",
    "\n",
    "train_fpr, train_tpr, _ = metrics.roc_curve(y_train, train_pred)\n",
    "test_fpr, test_tpr, _ = metrics.roc_curve(y_test, test_pred)\n",
    "\n",
    "# plot the roc curve for the model\n",
    "plt.plot(train_fpr, train_tpr, linestyle='--', label='Train')\n",
    "plt.plot(test_fpr, test_tpr, marker='.', label='Test')\n",
    "# axis labels\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "# show the legend\n",
    "plt.legend()\n",
    "# show the plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "B5v06kMj3WG2"
   },
   "source": [
    "We can also look at threshold-based metrics. By selecting a decision threshold $t$, we predict all observations with a predicted probability $\\hat{y} \\geq t$ to have an elevator and all others to have no elevator. \n",
    "\n",
    "Let's calculate the training and testing accuracy for our model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "rGPxVWcU3WG2",
    "outputId": "630dacad-5b3b-4020-974c-e11774407226",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "t = 0.5\n",
    "print(\"Train Accuracy: %.3f\" %  metrics.accuracy_score(y_train, train_pred > t))\n",
    "print(\"Test Accuracy: %.3f\" %  metrics.accuracy_score(y_test, test_pred > t))\n",
    "\n",
    "# If we wanted to get all of the elements of the confusion matrix, we could pull them like this:\n",
    "tn, fp, fn, tp = metrics.confusion_matrix(y_train, train_pred > t).ravel()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "00W37yL53WG2"
   },
   "source": [
    "**Question:** What is the baseline accuracy for this prediction task?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Answer:** The training set has 591/2449 = 24.1% elevators. A naive baseline model would predict the most common class, \"no elevators\" for all observations. This would be correct 75.9\\% of the time on the training set. The testing set also has elevators in 24.2\\% of units, so it would have 75.8\\% accuracy. (Remember that we stratified our sample, so it is not coincidental that the percentages are the same!)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.mean(1-y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_won2hl93WG3"
   },
   "source": [
    "Before we move on, we'll wrap all of our evaluation steps into a function that we can use to evaluate other models in a consistent way."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "tUUCnb_S3WG3"
   },
   "outputs": [],
   "source": [
    "def classification_evaluation(model, X_train, y_train, X_test, y_test, t = 0.5):\n",
    "    train_pred = model.predict_proba(X_train)[:,1]\n",
    "    test_pred = model.predict_proba(X_test)[:,1]\n",
    "\n",
    "    print(\"Train AUC: %.3f\" % metrics.roc_auc_score(y_train, train_pred))\n",
    "    print(\"Test AUC: %.3f\" % metrics.roc_auc_score(y_test, test_pred))\n",
    "    \n",
    "    print(\"\\nTrain Accuracy: %.3f\" %  metrics.accuracy_score(y_train, train_pred > t))\n",
    "    print(\"Test Accuracy: %.3f\" %  metrics.accuracy_score(y_test, test_pred > t))\n",
    "\n",
    "    train_fpr, train_tpr, _ = metrics.roc_curve(y_train, train_pred)\n",
    "    test_fpr, test_tpr, _ = metrics.roc_curve(y_test, test_pred)\n",
    "\n",
    "    # plot the roc curve for the model\n",
    "    plt.plot(train_fpr, train_tpr, linestyle='--', label='Train')\n",
    "    plt.plot(test_fpr, test_tpr, marker='.', label='Test')\n",
    "    # axis labels\n",
    "    plt.xlabel('False Positive Rate')\n",
    "    plt.ylabel('True Positive Rate')\n",
    "    # show the legend\n",
    "    plt.legend()\n",
    "    # show the plot\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "BPnWxuMH3WG3",
    "outputId": "6582cc85-2f02-4ab6-fe38-b35835979efc",
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "classification_evaluation(m_lr, X_train, y_train, X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3lhr0ypO3WG3"
   },
   "source": [
    "As you can see, `sklearn.metrics` is versatile and allows you to calculate and plot a bunch of different performance metrics. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "k1-VXmzG3WG3"
   },
   "source": [
    "### Classification Trees \n",
    "We will briefly explore classification trees.\n",
    "\n",
    "A (binary) classification tree makes predictions by grouping similar observations and then assigning a probability to each group using the proportion of observations within that group that belong to the positive class. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "s4m2Tu4a3WG3",
    "outputId": "ca0f74b2-330d-40d1-da90-2abb1f450b79"
   },
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "# Define the grid that we want to search over\n",
    "param_grid = {\"max_depth\": np.arange(3,10,1), \"criterion\": ['gini', 'entropy']}\n",
    "\n",
    "# Define the parameters for the model \n",
    "gs = GridSearchCV(DecisionTreeClassifier(random_state=42),\n",
    "                  return_train_score=True, \n",
    "                  param_grid=param_grid, \n",
    "                  scoring='roc_auc',\n",
    "                  cv=5, verbose = 0)\n",
    "\n",
    "## Fit the model\n",
    "random.seed(1)\n",
    "gs.fit(X_train, y_train)\n",
    "m_cart = gs.best_estimator_\n",
    "print(\"Best parameters: \", gs.best_params_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AkgK5mlg3WG3"
   },
   "source": [
    "We can plot and save the resulting tree as follows:\t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ZI4AKf8r3WG3",
    "outputId": "e3c1fdb9-3b71-4b80-9f3a-e998d6e1cef9",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from sklearn.tree import plot_tree\n",
    "\n",
    "fig, axes = plt.subplots(nrows = 1,ncols = 1,figsize = (50,10), dpi=200)\n",
    "plot_tree(m_cart, feature_names = X_train.columns, filled = False, fontsize = 8);\n",
    "fig.savefig('cart_tree_clf.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6tap8CpP3WG3"
   },
   "source": [
    "Let's use our evaluation function from before to see how well this model performs! "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "xSkMVINS3WG3",
    "outputId": "be9f83b7-24e0-4601-fc4a-9577dd0f4235"
   },
   "outputs": [],
   "source": [
    "classification_evaluation(m_cart, X_train, y_train, X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pF-pcWYt3WG3"
   },
   "source": [
    "The CART model does worse than the logistic regression model, with an AUC of 0.870 (vs. 0.9) on the test set."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WlFU_vel3WG3"
   },
   "source": [
    "### Random Forests\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bE8WQ0nQ3WG3"
   },
   "source": [
    "Let's start by training a random forest model for a classification task. We will perform the same task of predicting whether or not a listing has an elevator, using price and neighborhood as predictors. We will compare the performance of random forest to what we got using our simple CART model. We'll use a grid search to tune the number of estimators.\n",
    "\n",
    "*Note: This will take a while!*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "1TRw1R8b3WG3",
    "outputId": "ca486e2e-684c-42a6-e981-c26c14fd68d6"
   },
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "# Define the parameter grid\n",
    "param_grid = {\n",
    "    'n_estimators': [100,200,300],\n",
    "     'max_depth': np.arange(3,8,1),\n",
    "     'max_features': ['auto'],\n",
    "     'min_samples_leaf': [0.01,0.02],\n",
    "    'criterion' :['gini']\n",
    "}\n",
    "        \n",
    "# Define the parameters for the model \n",
    "gs = GridSearchCV(RandomForestClassifier(random_state=42),\n",
    "                  return_train_score=True, \n",
    "                  param_grid=param_grid, \n",
    "                  scoring='roc_auc',\n",
    "                  cv=5, verbose = 0)\n",
    "\n",
    "## Fit the model\n",
    "random.seed(1)\n",
    "gs.fit(X_train, y_train)\n",
    "m_rf = gs.best_estimator_\n",
    "print(\"Best parameters: \", gs.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "H1287T_73WG4",
    "outputId": "0f463cb0-21f9-4323-915b-6532b3334b6e",
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Let's evaluate the model performance\n",
    "classification_evaluation(m_rf, X_train, y_train, X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HkW8pMB13WG4"
   },
   "source": [
    "In general, random forests give stronger performance than single decision trees, but there is a sacrifice in interpretability. Interestingly, the logistic regression model is slightly better than the random forest model. The prediction accuracy is now much better, and it doesn't look like we are overfitting *too* much. It would likely be even better if the other hyperparameters of the random forest model were properly tuned! "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Nqk12D13kPWN"
   },
   "source": [
    "### Boosted Trees"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "VeNCtMf65KYe",
    "outputId": "90f1bbc0-cf3f-43a6-8580-6d3e9645ee93"
   },
   "outputs": [],
   "source": [
    "import xgboost as xgb\n",
    "import time\n",
    "\n",
    "# Define the parameter grid\n",
    "param_grid = {\n",
    "    'n_estimators': [100,200],\n",
    "     'max_depth': np.arange(2,6,1),     \n",
    "}\n",
    "        \n",
    "# Define the parameters for the model \n",
    "gs = GridSearchCV(xgb.XGBClassifier(random_state=42, use_label_encoder=False, eval_metric='logloss'),\n",
    "                  return_train_score=True, \n",
    "                  param_grid=param_grid, \n",
    "                  scoring='roc_auc',\n",
    "                  cv=5, verbose = 0)\n",
    "\n",
    "## Fit the model\n",
    "random.seed(1)\n",
    "start_time = time.time()\n",
    "gs.fit(X_train, y_train)\n",
    "print(\"--- %s seconds ---\" % (time.time() - start_time))\n",
    "\n",
    "m_xgb = gs.best_estimator_\n",
    "print(\"Best parameters: \", gs.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 364
    },
    "id": "SLpJ5SEQ63Tz",
    "outputId": "1bd32df9-14a7-45e4-ea2d-e3d11b0edcb4"
   },
   "outputs": [],
   "source": [
    "classification_evaluation(m_xgb, X_train, y_train, X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1wMVXcL2cYeX"
   },
   "source": [
    "LightGBM, short for Light Gradient Boosting Machine, is another distributed gradient boosting framework originally developed by Microsoft. Light GBM uses leaf-wise splitting over depth-wise splitting which enables it to converge much faster but also leads to a higher tendency to overfit."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "8IoIYGwr6_KH",
    "outputId": "097d920c-0a4a-4062-ddf2-0bd6aa6fade2",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import lightgbm as lgb\n",
    "\n",
    "# Define the parameter grid\n",
    "param_grid = {\n",
    "    'n_estimators': [100,150],\n",
    "     'max_depth': np.arange(2,6,1),\n",
    "}\n",
    "        \n",
    "# Define the parameters for the model \n",
    "gs = GridSearchCV(lgb.LGBMClassifier(random_state=42),\n",
    "                  return_train_score=True, \n",
    "                  param_grid=param_grid, \n",
    "                  scoring='roc_auc',\n",
    "                  cv=5, verbose = 0)\n",
    "\n",
    "## Fit the model\n",
    "random.seed(1)\n",
    "\n",
    "start_time = time.time()\n",
    "gs.fit(X_train, y_train)\n",
    "print(\"--- %s seconds ---\" % (time.time() - start_time))\n",
    "\n",
    "m_lgb = gs.best_estimator_\n",
    "print(\"Best parameters: \", gs.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 364
    },
    "id": "s9rLHxv67da3",
    "outputId": "b011591f-7c95-4ec2-e2ab-c9fabe8cfbd4"
   },
   "outputs": [],
   "source": [
    "classification_evaluation(m_lgb, X_train, y_train, X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FzU79VLmkbSi"
   },
   "source": [
    "### Neural Networks\n",
    "\n",
    "We won't cover neural networks during this lecture, Angelos and Leo will handle that in detail in the next lectures! Let's a see only a toy example .\n",
    "\n",
    "In general, we use TensorFlow or Pytorch to implement neural-networks but scikit-learn includes a basic implementation for feedforward neural networks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "a_KOFxfg8VM2",
    "outputId": "d935f83d-cdbf-4292-da6e-f96e63219780"
   },
   "outputs": [],
   "source": [
    "from sklearn.neural_network import MLPClassifier\n",
    "\n",
    "# Define the parameter grid\n",
    "param_grid = {\n",
    "     'hidden_layer_sizes': [(128), (128,64)], # the number of nodes at the each layer\n",
    "     'alpha': [1e-3, 1e-4] # regularization term, aka penalty term, that combats overfitting\n",
    "}\n",
    "\n",
    "#It is very important to scale the data for neural networks.\n",
    "std_scaler = StandardScaler()\n",
    "std_scaler.fit(X_train)\n",
    "X_train_standardized2 = std_scaler.transform(X_train)\n",
    "X_test_standardized2 = std_scaler.transform(X_test)\n",
    "        \n",
    "# Define the parameters for the model \n",
    "gs = GridSearchCV(MLPClassifier(random_state=42),\n",
    "                  return_train_score=True, \n",
    "                  param_grid=param_grid, \n",
    "                  scoring='roc_auc',\n",
    "                  cv=5, verbose = 1, n_jobs = -1)\n",
    "\n",
    "## Fit the model\n",
    "random.seed(42)\n",
    "gs.fit(X_train_standardized2, y_train)\n",
    "m_mlp = gs.best_estimator_\n",
    "print(\"Best parameters: \", gs.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "NbG1NzwigaHX",
    "outputId": "f749e7af-b855-4859-bdb2-649da080b6ad"
   },
   "outputs": [],
   "source": [
    "classification_evaluation(m_mlp, X_train_standardized2, y_train, X_test_standardized2, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UzsMKBPxk7Si"
   },
   "source": [
    "# Part 3: Advanced Topics \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "h-QR0EvrZWib"
   },
   "source": [
    "## A. Interpretability / Explainability\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mdweYIgP3WG4"
   },
   "source": [
    "#### Interpreting Random Forest and Boosted-Tree Models\n",
    "Although random forest and boosted-tree models are not very interpretable and hard to visualize, there is a popular method called *variable importance* that is commonly used. We will use the built-in `feature_importances_` from these models. This tells us which variables the random forest or boosted-tree model has determined as the most important for predicting whether or not a listing contains an elevator. It uses (usually) Gini importance, which is the mean decrease in node impurity (you can read more about this metric online), to rank the importance of all of the predictors used in the model. There are other metrics besides the Gini Importance that can also be used as a metric."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "zFTNMs0SXLpz",
    "outputId": "45853f67-9e8c-49e9-b346-b93ac218f209"
   },
   "outputs": [],
   "source": [
    "#Random Forest\n",
    "importance_output = pd.DataFrame({'feature':X_train.columns,\n",
    "                                 'importance':m_rf.feature_importances_}).sort_values('importance', ascending = False)\n",
    "\n",
    "plt.barh(importance_output['feature'][0:10], importance_output['importance'][0:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ZtFIeUFf3WG4",
    "outputId": "3488a2f5-0859-4a15-a852-f762edbbd480"
   },
   "outputs": [],
   "source": [
    "#XGBoost\n",
    "importance_output = pd.DataFrame({'feature':X_train.columns,\n",
    "                                 'importance':m_xgb.feature_importances_}).sort_values('importance', ascending = False)\n",
    "\n",
    "plt.barh(importance_output['feature'][0:10], importance_output['importance'][0:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Ac4Ke1qKtjG5",
    "outputId": "bccbc3eb-8110-4e5a-ca9c-34527814ae18"
   },
   "outputs": [],
   "source": [
    "#LightGBM\n",
    "importance_output = pd.DataFrame({'feature':X_train.columns,\n",
    "                                 'importance':m_lgb.feature_importances_}).sort_values('importance', ascending = False)\n",
    "\n",
    "plt.barh(importance_output['feature'][0:10], importance_output['importance'][0:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KK9cdb433WG4"
   },
   "source": [
    "For XGBoost, the variable importance plot shows us that whether or not the unit has a gym, how many listings the host has, and whether there is a doorman are the most important features. \n",
    "\n",
    "This not only gives us intuition about our models, but can also be used to select variables to train other models. However, there are some shortcomings. For example, price is important in the LightGBM model, but what is the direction of this relationship? Are high or low prices associated with elevator likelihood?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lo1JSE5G3WG4"
   },
   "source": [
    "#### Interpretation with SHapley Additive exPlanations (SHAP)\n",
    "There has been a lot of research devoted to better methods of interpreting \"black box\" machine learning models, like random forests. The [SHAP](https://shap.readthedocs.io/en/latest/) package offers an easy-to-use interface that estimates the contribution of each variable to each observation's prediction. This gives insight into both the *magnitude* and *directionality* of a feature's importance. The package also allows for plots of interactions between variables to understand important nonlinearities. We'll do this for the XGBoost model, but we could use the same package for many other models (including neural networks)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "KuKSWruK3WG4"
   },
   "outputs": [],
   "source": [
    "import shap  ## you can install this in terminal: 'conda install -c conda-forge shap'\n",
    "\n",
    "explainer = shap.TreeExplainer(m_xgb); # data=test_x, model_output=\"probability\");\n",
    "shap_values = explainer.shap_values(X_train)#[1]; # index 1 pulls the P(y=1) SHAP values for RandomForest."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "EUqYYo9N3WG4"
   },
   "source": [
    "##### Summary Plot\n",
    "We'll start with the summary plot, which ranks the features by importance (mean absolute SHAP value) and also shows the direction of the relationship. \n",
    "\n",
    "Features are ordered by decreasing significance, with the most important feature listed at the top of the plot. For a given feature, the corresponnding row shows a plot of the feature's impact on the prediction as the value ranges from its lowest (blue) to highest (red) value. Higher SHAP values correspond to increased likelihood of having a positive outcome (i.e. having an elevator). Thus, features with the color scale oriented blue to red (moving left to right) have increasing risk as the feature increases, such as hotel count. Note: the categorical variables are encoded as a binary value (e.g. 0=no gym, 1=gym), so \"higher\" values correspond to having the indicated feature."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "TIhN7Hdn3WG4",
    "outputId": "965a8298-0bb2-42bd-c217-fc371d1a11c0"
   },
   "outputs": [],
   "source": [
    "plt.close()\n",
    "shap.summary_plot(shap_values, X_train, show=True,\n",
    "                  max_display=10,\n",
    "                  plot_type=\"violin\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kwRwslCv3WG4"
   },
   "source": [
    "The summary plot gives us insight into the key driving features; this is similar to Gini feature importance but gives us more granular insight.\n",
    "- Gym, Doorman, Wheelchair Accessible,... all increase the probability of an elevator.\n",
    "- As price increases, the probability of an elevator increases.\n",
    "- Listings from hosts with more listings have higher probability of an elevator.\n",
    "- The property type being a house decreases the probability.\n",
    "\n",
    "##### Dependence Plots\n",
    "\n",
    "Let's dive into the relationship between price and whether the unit is predicted to have an elevator. We can create a dependence plot or price to see how varied prices affect the output probability."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "421Ugnoz3WG4",
    "outputId": "59bfb8dc-4326-4508-f4a2-bcb74f623c1c",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "plt.close()\n",
    "shap.dependence_plot('price', shap_values, X_train,\n",
    "                       interaction_index = None,\n",
    "                       xmin=\"percentile(1)\", xmax=\"percentile(99)\",\n",
    "                       dot_size=3,show=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8R8NMY113WG4"
   },
   "source": [
    "It looks like for the listings below 180/night, the price lowers the probability that the unit will have an elevator (SHAP value < 0). Above 180/night, there is a positive impact on probability of having an elevator. Above 300/night or so, however, the impact on the probability stabilizes; the probability does not continue to increase with prices above 300. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zcGJc6dClhjb"
   },
   "source": [
    "## B. Ensemble Modeling\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Q7Wkq3Njl5qI"
   },
   "source": [
    "Our goal is to obtain additional improvement from the top previous models. A very successful technique, widely used in data science competitions, is to build an ensemble model, i.e., using a combination (consensus) of different \"base learners\" to make a final decision. \n",
    "\n",
    "The general principle is to:\n",
    "- create train/val/test sets. Note: It is important to create a validation set.\n",
    "- fit a few different models on the Training set, that we evaluate on the Validation set\n",
    "- train a consensus model using the predictions of these models on the Validation set (which is unseen data) \n",
    "- evaluate the performance on the Test set\n",
    "\n",
    "**Question**: Why is it important to maintain a validation data to build an ensemble model?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "taS_EK13nSKj"
   },
   "outputs": [],
   "source": [
    "#We keep our train/test set from previously but we separate our train set into train and val.\n",
    "X_train_ensemble, X_val_ensemble, y_train_ensemble, y_val_ensemble = train_test_split(X_train, y_train, \n",
    "                                                    train_size = 0.75, random_state = 6,\n",
    "                                                   stratify = y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Xn0FojdRoG4Z"
   },
   "outputs": [],
   "source": [
    "#We previously trained several models. For prototyping sake, we don't do our validation or cross-validation again, but you always should in practice!\n",
    "\n",
    "#Get predictions on unseen data\n",
    "def create_prediction_data(model_list, X_train, y_train, X_val, y_val, X_test, y_test, verbose=False):\n",
    "    df_val = pd.DataFrame()\n",
    "    df_test = pd.DataFrame()\n",
    "    for key in model_list:\n",
    "        model_list[key].fit(X_train, y_train)\n",
    "        df_val[key] = model_list[key].predict_proba(X_val)[:,1]\n",
    "        df_test[key] = model_list[key].predict_proba(X_test)[:,1]\n",
    "        if verbose:\n",
    "            print(\"\\n#### \" + key +  \" ####\")\n",
    "            print(\"Test AUC: \", metrics.roc_auc_score(y_test, model_list[key].predict_proba(X_test)[:,1]))\n",
    "            print(\"Test Acc: \", metrics.accuracy_score(y_test, model_list[key].predict(X_test)))\n",
    "        #classification_evaluation(model_list[key], X_train, y_train, X_test, y_test)\n",
    "    return df_val, df_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "PLvJKJtPeSy-",
    "outputId": "d2ca5c50-237a-4096-e8a4-df28b32e4698"
   },
   "outputs": [],
   "source": [
    "#We get our ensemble data\n",
    "d_1 = {'logreg': m_lr, 'cart': m_cart, 'rf': m_rf, 'xgb': m_xgb, 'lgb': m_lgb}\n",
    "d_2 = {'logreg': m_lr, 'rf': m_rf, 'xgb': m_xgb, 'lgb': m_lgb}\n",
    "d_3 = {'logreg': m_lr, 'rf': m_rf, 'xgb': m_xgb, 'cart': m_cart}\n",
    "\n",
    "df_val_1, df_test_1 = create_prediction_data(d_1, X_train_ensemble, y_train_ensemble, X_val_ensemble, y_val_ensemble, X_test, y_test, verbose = True)\n",
    "df_val_2, df_test_2 = create_prediction_data(d_2, X_train_ensemble, y_train_ensemble, X_val_ensemble, y_val_ensemble, X_test, y_test)\n",
    "df_val_3, df_test_3 = create_prediction_data(d_3, X_train_ensemble, y_train_ensemble, X_val_ensemble, y_val_ensemble, X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RUb8_3fno-AH"
   },
   "source": [
    "### Average Ensemble\n",
    "\n",
    "The simplest version of an ensemble model is the average of the base models' probability outputs. Another possibility is to make a \"vote\" among the base models and use the majority vote as our prediction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "xmzXL7iZopjp",
    "outputId": "23d21f69-5bf4-4847-bc11-a4f73ceb59ce"
   },
   "outputs": [],
   "source": [
    "print(\"\\n#### Probability average with CART included ####\")\n",
    "print(\"Test AUC: \", metrics.roc_auc_score(y_test, df_test_1.mean(axis = 1)))\n",
    "print(\"Test Acc: \", metrics.accuracy_score(y_test, df_test_1.mean(axis = 1) > 0.5))\n",
    "\n",
    "print(\"\\n#### Probability average without CART ####\")\n",
    "print(\"Test AUC: \", metrics.roc_auc_score(y_test, df_test_2.mean(axis = 1)))\n",
    "print(\"Test Acc: \", metrics.accuracy_score(y_test, df_test_2.mean(axis = 1) > 0.5))\n",
    "\n",
    "print(\"\\n#### Probability average without LightGBM ####\")\n",
    "print(\"Test AUC: \", metrics.roc_auc_score(y_test, df_test_3.mean(axis = 1)))\n",
    "print(\"Test Acc: \", metrics.accuracy_score(y_test, df_test_3.mean(axis = 1) > 0.5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tSrphRa2yYw_"
   },
   "source": [
    "### Weighted Average Ensemble\n",
    "\n",
    "A slightly more sophisticated version of an ensemble model is the weighted average of the base models' probability outputs. We can train a Lasso Logistic Regression to obtain the different weights."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "0tq0XqSdo_ve",
    "outputId": "185053cf-6a72-4578-e5d0-d2474a46a307"
   },
   "outputs": [],
   "source": [
    "# Weighted average\n",
    "# Define the grid that we want to search over\n",
    "param_grid = {'C': np.arange(0.001, 1, 0.05), \n",
    "              'penalty': ['l2','l1'], \n",
    "              'solver': ['liblinear']}\n",
    "\n",
    "# Define the parameters for the model \n",
    "gs_ensemble = GridSearchCV(LogisticRegression(random_state=42, max_iter = 1000),\n",
    "                  return_train_score=True, \n",
    "                  param_grid=param_grid, \n",
    "                  scoring='roc_auc',\n",
    "                  cv=5, verbose = 0)\n",
    "## Fit the model\n",
    "random.seed(1)\n",
    "gs_ensemble.fit(df_val_1, y_val_ensemble)\n",
    "m_lr_ensemble = gs_ensemble.best_estimator_\n",
    "print(\"Best parameters: \", gs_ensemble.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Vy9UYW8J46A_",
    "outputId": "52029648-2c2f-42f4-b299-3238291d30e2"
   },
   "outputs": [],
   "source": [
    "print(\"Test AUC: \", metrics.roc_auc_score(y_test, m_lr_ensemble.predict_proba(df_test_1)[:,1]))\n",
    "print(\"Test Acc: \", metrics.accuracy_score(y_test, m_lr_ensemble.predict(df_test_1)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ec4vzDjDtZt1",
    "outputId": "18fe0bb3-6cd7-458b-a7f9-e96723510170"
   },
   "outputs": [],
   "source": [
    "# Let's look at the coefficients\n",
    "coef_output = pd.DataFrame({'feature':df_test_1.columns,\n",
    "              'coefficient':m_lr_ensemble.coef_[0]})\n",
    "\n",
    "print(\"Number of zeros: %d\" % (coef_output.query('coefficient == 0').shape[0]))\n",
    "coef_output.sort_values('coefficient')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "B9nnGBF9tszY"
   },
   "source": [
    "The \"optimal\" weighted average ensemble leads to roughly the same top performance using only 3 out of the 5 models. This is extremely desired for deployment purposes as it means fewer models will need to be implemented which results in a faster and more reliable pipeline."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fshxU3oN461A"
   },
   "source": [
    "## C. Automatic ML\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IQGECgz-IJ4G"
   },
   "source": [
    "### Lazypredict\n",
    "\n",
    "Lazypredict is a package that trains the most popular machine learning models on your data, with their default hyperparameters values. It can be very useful to find out what are the top performing models before you start yourself the hyperparameter tuning.\n",
    "\n",
    "I also like to use it to obtain a set of base learners for an ensemble model. I generally keep the 10-15 out-of-the-box top models given by ```lazypredict``` and use a Lasso ensemble afterwards."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "oPMHP0TnKhXe",
    "outputId": "c78b2dd8-16ff-49f5-a6d3-09eb1e98c538"
   },
   "outputs": [],
   "source": [
    "import lazypredict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 412
    },
    "id": "pnr9s2Q44_ZL",
    "outputId": "a6c7efc4-1a4a-450b-b14d-5302895b108b"
   },
   "outputs": [],
   "source": [
    "#As usual, you can use the Regressor or Classifier version\n",
    "from lazypredict.Supervised import LazyRegressor\n",
    "from lazypredict.Supervised import LazyClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "9tKNGZ2VR1f4"
   },
   "outputs": [],
   "source": [
    "reg = LazyClassifier(predictions = True, verbose=0, ignore_warnings=True, custom_metric=None)\n",
    "models, predictions = reg.fit(X_train_ensemble, X_val_ensemble, y_train_ensemble, y_val_ensemble)\n",
    "\n",
    "#Performance is reported on the validation set\n",
    "models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YsrEsGQwlmRp"
   },
   "source": [
    "# Conclusion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "386hHXNr3WG4"
   },
   "source": [
    "### Model Selection\n",
    "**Question:** Now that we have trained and evaluated so many different models, which one would you select?\n",
    "\n",
    "**Answer:** It depends on the application, the stakeholders, and the costs of missed positives and negatives (or in continuous prediction, under- and over-predictions). \n",
    "\n",
    "In a problem like this, the prediction of whether an Airbnb listing will have an elevator is not highly consequential. However, consider a clinical decision support tool that predicts hospital re-admission or a model that predicts loan defaults. In such cases, there may be a higher premium on interpretability to ensure transparency for patients (or bank clients). \n",
    "\n",
    "On the other hand, a tool that is used to triage customer service calls within a company may not need to be as interpretable because it is used purely internally. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7eWiMyJ63WG4"
   },
   "source": [
    "## Take-aways\n",
    "In this module, we have covered examples of machine learning methods for regression (ordinary and penalized) and classification.  This is just the tip of the iceberg.  There are tons more machine learning methods which can be easily implemented in Python.  \n",
    "\n",
    "We haven't focused that much on the feature preprocessing and data cleaning (because it is boring?), but this is often one of the most important aspects. \n",
    "\n",
    "Remember to always scrutinize your data before doing serious modeling.\n",
    "\n",
    "In particular: \n",
    "- look for outliers, \n",
    "- handle missing data (e.g., delete, impute, interpolate),\n",
    "- observe the feature distributions,\n",
    "- observe the feature correlations,\n",
    "- consider engineering a few additional features \n",
    "- remember real-world data is always messy...\n",
    "\n",
    "During modeling:\n",
    "- always cross-validate or at least validate your hyperparameters,\n",
    "- do not try to overfit your test set!\n",
    "- think about deployment: will you have data drifts? Is your model easy to deploy? Does your model run fast?\n",
    "- choose your evaluation metrics carefully.\n",
    "\n",
    "To get some extra juice:\n",
    "- consider using ensemble modeling, but if possible keep it simple\n",
    "- Automatic ML can save you a lot of time, I wish I knew it before!!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IXLeWN8n3WG5"
   },
   "source": [
    "### Notes on sklearn modeling\n",
    "\n",
    "- There are many, many classification algorithms implemented in `scikit-learn`. [Here's](https://scikit-learn.org/stable/auto_examples/classification/plot_classifier_comparison.html) an example that compares a broad set of methods, including SVM, k-Nearest Neighbors, and others.\n",
    "- XGBoost and LightGBM follow nearly identical syntax to the examples above (using GridSearchCV). However, it is not implemented in scikit-learn. You have to install the [xgboost](https://xgboost.readthedocs.io/en/latest/) library and then adapt the code above to this model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Credits\n",
    "\n",
    "This Notebook is based on previous work by **Holly Wiberg, Léonard Boussioux, and Asterios Tsiourvas**. A **BIG** thank you to all of them!!!"
   ]
  }
 ],
 "metadata": {
  "@webio": {
   "lastCommId": null,
   "lastKernelId": null
  },
  "colab": {
   "collapsed_sections": [
    "kaPIKFLdN9wT",
    "f3yiVsGiKqWW",
    "eFrVh4fjK-Wu",
    "FAbR9wER3WGw",
    "wm5b35Ls3WGz",
    "1WzMOXyT3WG0",
    "XmOd_YEa3WG0",
    "cXTP_RXKZ5w3",
    "iNM_A4gI3WG1",
    "k1-VXmzG3WG3",
    "WlFU_vel3WG3",
    "FzU79VLmkbSi",
    "-AOhojDJkri7",
    "mdweYIgP3WG4",
    "lo1JSE5G3WG4",
    "zcGJc6dClhjb",
    "RUb8_3fno-AH",
    "tSrphRa2yYw_",
    "P-7QvR5HzMxT",
    "IQGECgz-IJ4G",
    "NP3M-HljR4LF",
    "YsrEsGQwlmRp",
    "386hHXNr3WG4",
    "IXLeWN8n3WG5"
   ],
   "name": "Intro2ML_Python_MBAn22.ipynb",
   "provenance": []
  },
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "12cdeecac7194c7cac2412ca2b58798a": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "411fe947a0d04955b4f7df5ce365c92c": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_904f759604014e529466fe2bf52c15ad",
      "max": 200,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_12cdeecac7194c7cac2412ca2b58798a",
      "value": 200
     }
    },
    "589bf77e735f45f9a046758484492f9c": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "724508596b48407589bbedc8d04a8305": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "7f8c1b4d50a343b9b7aa367b157cc604": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_589bf77e735f45f9a046758484492f9c",
      "placeholder": "​",
      "style": "IPY_MODEL_ec831ac0a8fb48efb205a69b709cd2a1",
      "value": " 201/? [25:11&lt;00:00,  7.49s/pipeline]"
     }
    },
    "904f759604014e529466fe2bf52c15ad": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "c22a859ab08542a2a0b5b643fe7ebebf": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_d73c89f564444cc8a1209bca0f15ca92",
       "IPY_MODEL_411fe947a0d04955b4f7df5ce365c92c",
       "IPY_MODEL_7f8c1b4d50a343b9b7aa367b157cc604"
      ],
      "layout": "IPY_MODEL_724508596b48407589bbedc8d04a8305"
     }
    },
    "cc2a566479bd44e2b1ffa172d9a28ca9": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "d73c89f564444cc8a1209bca0f15ca92": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_cc2a566479bd44e2b1ffa172d9a28ca9",
      "placeholder": "​",
      "style": "IPY_MODEL_e0fb2d28f0af4c1292c33b39bf248b44",
      "value": "Optimization Progress: "
     }
    },
    "e0fb2d28f0af4c1292c33b39bf248b44": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "ec831ac0a8fb48efb205a69b709cd2a1": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
